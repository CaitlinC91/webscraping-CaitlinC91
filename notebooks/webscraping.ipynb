{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "In this exercise, you'll practice using BeautifulSoup to parse the content of a web page. The page that you'll be scraping, https://realpython.github.io/fake-jobs/, contains job listings. Your job is to extract the data on each job and convert into a pandas DataFrame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Start by performing a GET request on the url above and convert the response into a BeautifulSoup object.  \n",
    "a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this title.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2 class=\"title is-5\">Senior Python Developer</h2>\n"
     ]
    }
   ],
   "source": [
    "results = soup.find(\"h2\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Python Developer\n"
     ]
    }
   ],
   "source": [
    "results = soup.find(\"h2\").text\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"title is-5\">Senior Python Developer</h2>,\n",
       " <h2 class=\"title is-5\">Energy engineer</h2>,\n",
       " <h2 class=\"title is-5\">Legal executive</h2>,\n",
       " <h2 class=\"title is-5\">Fitness centre manager</h2>,\n",
       " <h2 class=\"title is-5\">Product manager</h2>,\n",
       " <h2 class=\"title is-5\">Medical technical officer</h2>,\n",
       " <h2 class=\"title is-5\">Physiological scientist</h2>,\n",
       " <h2 class=\"title is-5\">Textile designer</h2>,\n",
       " <h2 class=\"title is-5\">Television floor manager</h2>,\n",
       " <h2 class=\"title is-5\">Waste management officer</h2>,\n",
       " <h2 class=\"title is-5\">Software Engineer (Python)</h2>,\n",
       " <h2 class=\"title is-5\">Interpreter</h2>,\n",
       " <h2 class=\"title is-5\">Architect</h2>,\n",
       " <h2 class=\"title is-5\">Meteorologist</h2>,\n",
       " <h2 class=\"title is-5\">Audiological scientist</h2>,\n",
       " <h2 class=\"title is-5\">English as a second language teacher</h2>,\n",
       " <h2 class=\"title is-5\">Surgeon</h2>,\n",
       " <h2 class=\"title is-5\">Equities trader</h2>,\n",
       " <h2 class=\"title is-5\">Newspaper journalist</h2>,\n",
       " <h2 class=\"title is-5\">Materials engineer</h2>,\n",
       " <h2 class=\"title is-5\">Python Programmer (Entry-Level)</h2>,\n",
       " <h2 class=\"title is-5\">Product/process development scientist</h2>,\n",
       " <h2 class=\"title is-5\">Scientist, research (maths)</h2>,\n",
       " <h2 class=\"title is-5\">Ecologist</h2>,\n",
       " <h2 class=\"title is-5\">Materials engineer</h2>,\n",
       " <h2 class=\"title is-5\">Historic buildings inspector/conservation officer</h2>,\n",
       " <h2 class=\"title is-5\">Data scientist</h2>,\n",
       " <h2 class=\"title is-5\">Psychiatrist</h2>,\n",
       " <h2 class=\"title is-5\">Structural engineer</h2>,\n",
       " <h2 class=\"title is-5\">Immigration officer</h2>,\n",
       " <h2 class=\"title is-5\">Python Programmer (Entry-Level)</h2>,\n",
       " <h2 class=\"title is-5\">Neurosurgeon</h2>,\n",
       " <h2 class=\"title is-5\">Broadcast engineer</h2>,\n",
       " <h2 class=\"title is-5\">Make</h2>,\n",
       " <h2 class=\"title is-5\">Nurse, adult</h2>,\n",
       " <h2 class=\"title is-5\">Air broker</h2>,\n",
       " <h2 class=\"title is-5\">Editor, film/video</h2>,\n",
       " <h2 class=\"title is-5\">Production assistant, radio</h2>,\n",
       " <h2 class=\"title is-5\">Engineer, communications</h2>,\n",
       " <h2 class=\"title is-5\">Sales executive</h2>,\n",
       " <h2 class=\"title is-5\">Software Developer (Python)</h2>,\n",
       " <h2 class=\"title is-5\">Futures trader</h2>,\n",
       " <h2 class=\"title is-5\">Tour manager</h2>,\n",
       " <h2 class=\"title is-5\">Cytogeneticist</h2>,\n",
       " <h2 class=\"title is-5\">Designer, multimedia</h2>,\n",
       " <h2 class=\"title is-5\">Trade union research officer</h2>,\n",
       " <h2 class=\"title is-5\">Chemist, analytical</h2>,\n",
       " <h2 class=\"title is-5\">Programmer, multimedia</h2>,\n",
       " <h2 class=\"title is-5\">Engineer, broadcasting (operations)</h2>,\n",
       " <h2 class=\"title is-5\">Teacher, primary school</h2>,\n",
       " <h2 class=\"title is-5\">Python Developer</h2>,\n",
       " <h2 class=\"title is-5\">Manufacturing systems engineer</h2>,\n",
       " <h2 class=\"title is-5\">Producer, television/film/video</h2>,\n",
       " <h2 class=\"title is-5\">Scientist, forensic</h2>,\n",
       " <h2 class=\"title is-5\">Bonds trader</h2>,\n",
       " <h2 class=\"title is-5\">Editorial assistant</h2>,\n",
       " <h2 class=\"title is-5\">Photographer</h2>,\n",
       " <h2 class=\"title is-5\">Retail banker</h2>,\n",
       " <h2 class=\"title is-5\">Jewellery designer</h2>,\n",
       " <h2 class=\"title is-5\">Ophthalmologist</h2>,\n",
       " <h2 class=\"title is-5\">Back-End Web Developer (Python, Django)</h2>,\n",
       " <h2 class=\"title is-5\">Licensed conveyancer</h2>,\n",
       " <h2 class=\"title is-5\">Futures trader</h2>,\n",
       " <h2 class=\"title is-5\">Counselling psychologist</h2>,\n",
       " <h2 class=\"title is-5\">Insurance underwriter</h2>,\n",
       " <h2 class=\"title is-5\">Engineer, automotive</h2>,\n",
       " <h2 class=\"title is-5\">Producer, radio</h2>,\n",
       " <h2 class=\"title is-5\">Dispensing optician</h2>,\n",
       " <h2 class=\"title is-5\">Designer, fashion/clothing</h2>,\n",
       " <h2 class=\"title is-5\">Chartered loss adjuster</h2>,\n",
       " <h2 class=\"title is-5\">Back-End Web Developer (Python, Django)</h2>,\n",
       " <h2 class=\"title is-5\">Forest/woodland manager</h2>,\n",
       " <h2 class=\"title is-5\">Clinical cytogeneticist</h2>,\n",
       " <h2 class=\"title is-5\">Print production planner</h2>,\n",
       " <h2 class=\"title is-5\">Systems developer</h2>,\n",
       " <h2 class=\"title is-5\">Graphic designer</h2>,\n",
       " <h2 class=\"title is-5\">Writer</h2>,\n",
       " <h2 class=\"title is-5\">Field seismologist</h2>,\n",
       " <h2 class=\"title is-5\">Chief Strategy Officer</h2>,\n",
       " <h2 class=\"title is-5\">Air cabin crew</h2>,\n",
       " <h2 class=\"title is-5\">Python Programmer (Entry-Level)</h2>,\n",
       " <h2 class=\"title is-5\">Warden/ranger</h2>,\n",
       " <h2 class=\"title is-5\">Sports therapist</h2>,\n",
       " <h2 class=\"title is-5\">Arts development officer</h2>,\n",
       " <h2 class=\"title is-5\">Printmaker</h2>,\n",
       " <h2 class=\"title is-5\">Health and safety adviser</h2>,\n",
       " <h2 class=\"title is-5\">Manufacturing systems engineer</h2>,\n",
       " <h2 class=\"title is-5\">Programmer, applications</h2>,\n",
       " <h2 class=\"title is-5\">Medical physicist</h2>,\n",
       " <h2 class=\"title is-5\">Media planner</h2>,\n",
       " <h2 class=\"title is-5\">Software Developer (Python)</h2>,\n",
       " <h2 class=\"title is-5\">Surveyor, land/geomatics</h2>,\n",
       " <h2 class=\"title is-5\">Legal executive</h2>,\n",
       " <h2 class=\"title is-5\">Librarian, academic</h2>,\n",
       " <h2 class=\"title is-5\">Barrister</h2>,\n",
       " <h2 class=\"title is-5\">Museum/gallery exhibitions officer</h2>,\n",
       " <h2 class=\"title is-5\">Radiographer, diagnostic</h2>,\n",
       " <h2 class=\"title is-5\">Database administrator</h2>,\n",
       " <h2 class=\"title is-5\">Furniture designer</h2>,\n",
       " <h2 class=\"title is-5\">Ship broker</h2>]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobtitles = soup.findAll(\"h2\")\n",
    "jobtitles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senior Python Developer', 'Energy engineer', 'Legal executive', 'Fitness centre manager', 'Product manager', 'Medical technical officer', 'Physiological scientist', 'Textile designer', 'Television floor manager', 'Waste management officer', 'Software Engineer (Python)', 'Interpreter', 'Architect', 'Meteorologist', 'Audiological scientist', 'English as a second language teacher', 'Surgeon', 'Equities trader', 'Newspaper journalist', 'Materials engineer', 'Python Programmer (Entry-Level)', 'Product/process development scientist', 'Scientist, research (maths)', 'Ecologist', 'Materials engineer', 'Historic buildings inspector/conservation officer', 'Data scientist', 'Psychiatrist', 'Structural engineer', 'Immigration officer', 'Python Programmer (Entry-Level)', 'Neurosurgeon', 'Broadcast engineer', 'Make', 'Nurse, adult', 'Air broker', 'Editor, film/video', 'Production assistant, radio', 'Engineer, communications', 'Sales executive', 'Software Developer (Python)', 'Futures trader', 'Tour manager', 'Cytogeneticist', 'Designer, multimedia', 'Trade union research officer', 'Chemist, analytical', 'Programmer, multimedia', 'Engineer, broadcasting (operations)', 'Teacher, primary school', 'Python Developer', 'Manufacturing systems engineer', 'Producer, television/film/video', 'Scientist, forensic', 'Bonds trader', 'Editorial assistant', 'Photographer', 'Retail banker', 'Jewellery designer', 'Ophthalmologist', 'Back-End Web Developer (Python, Django)', 'Licensed conveyancer', 'Futures trader', 'Counselling psychologist', 'Insurance underwriter', 'Engineer, automotive', 'Producer, radio', 'Dispensing optician', 'Designer, fashion/clothing', 'Chartered loss adjuster', 'Back-End Web Developer (Python, Django)', 'Forest/woodland manager', 'Clinical cytogeneticist', 'Print production planner', 'Systems developer', 'Graphic designer', 'Writer', 'Field seismologist', 'Chief Strategy Officer', 'Air cabin crew', 'Python Programmer (Entry-Level)', 'Warden/ranger', 'Sports therapist', 'Arts development officer', 'Printmaker', 'Health and safety adviser', 'Manufacturing systems engineer', 'Programmer, applications', 'Medical physicist', 'Media planner', 'Software Developer (Python)', 'Surveyor, land/geomatics', 'Legal executive', 'Librarian, academic', 'Barrister', 'Museum/gallery exhibitions officer', 'Radiographer, diagnostic', 'Database administrator', 'Furniture designer', 'Ship broker']\n"
     ]
    }
   ],
   "source": [
    "jobtitles_text = [] \n",
    "\n",
    "for jobtitle in jobtitles:\n",
    "    jobtitle_text = jobtitle.text.strip()\n",
    "    jobtitles_text.append(jobtitle_text)  \n",
    "\n",
    "print(jobtitles_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m companies \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfindAll(\u001b[39m\"\u001b[39m\u001b[39mh3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m locations \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfindAll(\u001b[39m\"\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m time \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfindALL(\u001b[39m\"\u001b[39;49m\u001b[39mp\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m2021-04-08\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "companies = soup.findAll(\"h3\")\n",
    "locations = soup.findAll(\"p\", \"location\")\n",
    "time = soup.findALL(\"p\", \"2021-04-08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Payne, Roberts and Davis',\n",
       " 'Vasquez-Davidson',\n",
       " 'Jackson, Chambers and Levy',\n",
       " 'Savage-Bradley',\n",
       " 'Ramirez Inc',\n",
       " 'Rogers-Yates',\n",
       " 'Kramer-Klein',\n",
       " 'Meyers-Johnson',\n",
       " 'Hughes-Williams',\n",
       " 'Jones, Williams and Villa',\n",
       " 'Garcia PLC',\n",
       " 'Gregory and Sons',\n",
       " 'Clark, Garcia and Sosa',\n",
       " 'Bush PLC',\n",
       " 'Salazar-Meyers',\n",
       " 'Parker, Murphy and Brooks',\n",
       " 'Cruz-Brown',\n",
       " 'Macdonald-Ferguson',\n",
       " 'Williams, Peterson and Rojas',\n",
       " 'Smith and Sons',\n",
       " 'Moss, Duncan and Allen',\n",
       " 'Gomez-Carroll',\n",
       " 'Manning, Welch and Herring',\n",
       " 'Lee, Gutierrez and Brown',\n",
       " 'Davis, Serrano and Cook',\n",
       " 'Smith LLC',\n",
       " 'Thomas Group',\n",
       " 'Silva-King',\n",
       " 'Pierce-Long',\n",
       " 'Walker-Simpson',\n",
       " 'Cooper and Sons',\n",
       " 'Donovan, Gonzalez and Figueroa',\n",
       " 'Morgan, Butler and Bennett',\n",
       " 'Snyder-Lee',\n",
       " 'Harris PLC',\n",
       " 'Washington PLC',\n",
       " 'Brown, Price and Campbell',\n",
       " 'Mcgee PLC',\n",
       " 'Dixon Inc',\n",
       " 'Thompson, Sheppard and Ward',\n",
       " 'Adams-Brewer',\n",
       " 'Schneider-Brady',\n",
       " 'Gonzales-Frank',\n",
       " 'Smith-Wong',\n",
       " 'Pierce-Herrera',\n",
       " 'Aguilar, Rivera and Quinn',\n",
       " 'Lowe, Barnes and Thomas',\n",
       " 'Lewis, Gonzalez and Vasquez',\n",
       " 'Taylor PLC',\n",
       " 'Oliver, Jones and Ramirez',\n",
       " 'Rivera and Sons',\n",
       " 'Garcia PLC',\n",
       " 'Johnson, Wells and Kramer',\n",
       " 'Gonzalez LLC',\n",
       " 'Morgan, White and Macdonald',\n",
       " 'Robinson-Fitzpatrick',\n",
       " 'Waters, Wilson and Hoover',\n",
       " 'Hill LLC',\n",
       " 'Li-Gregory',\n",
       " 'Fisher, Ryan and Coleman',\n",
       " 'Stewart-Alexander',\n",
       " 'Abbott and Sons',\n",
       " 'Bryant, Santana and Davenport',\n",
       " 'Smith PLC',\n",
       " 'Patterson-Singh',\n",
       " 'Martinez-Berry',\n",
       " 'May, Taylor and Fisher',\n",
       " 'Bailey, Owen and Thompson',\n",
       " 'Vasquez Ltd',\n",
       " 'Leblanc LLC',\n",
       " 'Jackson, Ali and Mckee',\n",
       " 'Blankenship, Knight and Powell',\n",
       " 'Patton, Haynes and Jones',\n",
       " 'Wood Inc',\n",
       " 'Collins Group',\n",
       " 'Flores-Nelson',\n",
       " 'Mitchell, Jones and Olson',\n",
       " 'Howard Group',\n",
       " 'Kramer-Edwards',\n",
       " 'Berry-Houston',\n",
       " 'Mathews Inc',\n",
       " 'Riley-Johnson',\n",
       " 'Spencer and Sons',\n",
       " 'Camacho-Sanchez',\n",
       " 'Oliver and Sons',\n",
       " 'Eaton PLC',\n",
       " 'Stanley-Frederick',\n",
       " 'Bradley LLC',\n",
       " 'Parker, Goodwin and Zavala',\n",
       " 'Kim-Miles',\n",
       " 'Moreno-Rodriguez',\n",
       " 'Brown-Ortiz',\n",
       " 'Hartman PLC',\n",
       " 'Brooks Inc',\n",
       " 'Washington-Castillo',\n",
       " 'Nguyen, Yoder and Petty',\n",
       " 'Holder LLC',\n",
       " 'Yates-Ferguson',\n",
       " 'Ortega-Lawrence',\n",
       " 'Fuentes, Walls and Castro']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_text = [] \n",
    "\n",
    "for tx in companies:\n",
    "    x = tx.text.strip()\n",
    "    companies_text.append(x)  \n",
    "companies_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stewartbury, AA',\n",
       " 'Christopherville, AA',\n",
       " 'Port Ericaburgh, AA',\n",
       " 'East Seanview, AP',\n",
       " 'North Jamieview, AP',\n",
       " 'Davidville, AP',\n",
       " 'South Christopher, AE',\n",
       " 'Port Jonathan, AE',\n",
       " 'Osbornetown, AE',\n",
       " 'Scotttown, AP',\n",
       " 'Ericberg, AE',\n",
       " 'Ramireztown, AE',\n",
       " 'Figueroaview, AA',\n",
       " 'Kelseystad, AA',\n",
       " 'Williamsburgh, AE',\n",
       " 'Mitchellburgh, AE',\n",
       " 'West Jessicabury, AA',\n",
       " 'Maloneshire, AE',\n",
       " 'Johnsonton, AA',\n",
       " 'South Davidtown, AP',\n",
       " 'Port Sara, AE',\n",
       " 'Marktown, AA',\n",
       " 'Laurenland, AE',\n",
       " 'Lauraton, AP',\n",
       " 'South Tammyberg, AP',\n",
       " 'North Brandonville, AP',\n",
       " 'Port Robertfurt, AA',\n",
       " 'Burnettbury, AE',\n",
       " 'Herbertside, AA',\n",
       " 'Christopherport, AP',\n",
       " 'West Victor, AE',\n",
       " 'Port Aaron, AP',\n",
       " 'Loribury, AA',\n",
       " 'Angelastad, AP',\n",
       " 'Larrytown, AE',\n",
       " 'West Colin, AP',\n",
       " 'West Stephanie, AP',\n",
       " 'Laurentown, AP',\n",
       " 'Wrightberg, AP',\n",
       " 'Alberttown, AE',\n",
       " 'Brockburgh, AE',\n",
       " 'North Jason, AE',\n",
       " 'Arnoldhaven, AE',\n",
       " 'Lake Destiny, AP',\n",
       " 'South Timothyburgh, AP',\n",
       " 'New Jimmyton, AE',\n",
       " 'New Lucasbury, AP',\n",
       " 'Port Cory, AE',\n",
       " 'Gileston, AA',\n",
       " 'Cindyshire, AA',\n",
       " 'East Michaelfort, AA',\n",
       " 'Joybury, AE',\n",
       " 'Emmatown, AE',\n",
       " 'Colehaven, AP',\n",
       " 'Port Coryton, AE',\n",
       " 'Amyborough, AA',\n",
       " 'Reynoldsville, AA',\n",
       " 'Port Billy, AP',\n",
       " 'Adamburgh, AA',\n",
       " 'Wilsonmouth, AA',\n",
       " 'South Kimberly, AA',\n",
       " 'Benjaminland, AP',\n",
       " 'Zacharyport, AA',\n",
       " 'Port Devonville, AE',\n",
       " 'East Thomas, AE',\n",
       " 'New Jeffrey, AP',\n",
       " 'Davidside, AA',\n",
       " 'Jamesville, AA',\n",
       " 'New Kelly, AP',\n",
       " 'Lake Antonio, AA',\n",
       " 'New Elizabethside, AA',\n",
       " 'Millsbury, AE',\n",
       " 'Lloydton, AP',\n",
       " 'Port Jeremy, AA',\n",
       " 'New Elizabethtown, AA',\n",
       " 'Charlesstad, AE',\n",
       " 'Josephbury, AE',\n",
       " 'Seanfurt, AA',\n",
       " 'Williambury, AA',\n",
       " 'South Jorgeside, AP',\n",
       " 'Robertborough, AP',\n",
       " 'South Saratown, AP',\n",
       " 'Hullview, AA',\n",
       " 'Philipland, AP',\n",
       " 'North Patty, AE',\n",
       " 'North Stephen, AE',\n",
       " 'Stevensland, AP',\n",
       " 'Reyesstad, AE',\n",
       " 'Bellberg, AP',\n",
       " 'North Johnland, AE',\n",
       " 'Martinezburgh, AE',\n",
       " 'Joshuatown, AE',\n",
       " 'West Ericstad, AA',\n",
       " 'Tuckertown, AE',\n",
       " 'Perezton, AE',\n",
       " 'Lake Abigail, AE',\n",
       " 'Jacobshire, AP',\n",
       " 'Port Susan, AE',\n",
       " 'North Tiffany, AA',\n",
       " 'Michelleville, AP']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_text = [] \n",
    "\n",
    "for loca in locations:\n",
    "    x = loca.text.strip()\n",
    "    locations_text.append(x)  \n",
    "locations_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "time = soup.find_all(\"2021-04-08\")\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "SelectorSyntaxError",
     "evalue": "Malformed attribute selector at position 1\n  line 1:\np[datetime^=2021]\n ^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSelectorSyntaxError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m time \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mp[datetime^=2021]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\bs4\\element.py:1973\u001b[0m, in \u001b[0;36mTag.select\u001b[1;34m(self, selector, namespaces, limit, **kwargs)\u001b[0m\n\u001b[0;32m   1968\u001b[0m \u001b[39mif\u001b[39;00m soupsieve \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1969\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1970\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot execute CSS selectors because the soupsieve package is not installed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1971\u001b[0m     )\n\u001b[1;32m-> 1973\u001b[0m results \u001b[39m=\u001b[39m soupsieve\u001b[39m.\u001b[39mselect(selector, \u001b[39mself\u001b[39m, namespaces, limit, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1975\u001b[0m \u001b[39m# We do this because it's more consistent and because\u001b[39;00m\n\u001b[0;32m   1976\u001b[0m \u001b[39m# ResultSet.__getattr__ has a helpful error message.\u001b[39;00m\n\u001b[0;32m   1977\u001b[0m \u001b[39mreturn\u001b[39;00m ResultSet(\u001b[39mNone\u001b[39;00m, results)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\soupsieve\\__init__.py:144\u001b[0m, in \u001b[0;36mselect\u001b[1;34m(select, tag, namespaces, limit, flags, custom, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\n\u001b[0;32m    133\u001b[0m     select: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    134\u001b[0m     tag: \u001b[39m'\u001b[39m\u001b[39mbs4.Tag\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    141\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39m'\u001b[39m\u001b[39mbs4.Tag\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    142\u001b[0m     \u001b[39m\"\"\"Select the specified tags.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcompile\u001b[39m(select, namespaces, flags, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mselect(tag, limit)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\soupsieve\\__init__.py:67\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(pattern, namespaces, flags, custom, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot process \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcustom\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument on a compiled selector list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m pattern\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m cp\u001b[39m.\u001b[39;49m_cached_css_compile(pattern, ns, cs, flags)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py:222\u001b[0m, in \u001b[0;36m_cached_css_compile\u001b[1;34m(pattern, namespaces, custom, flags)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39m\"\"\"Cached CSS compile.\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m custom_selectors \u001b[39m=\u001b[39m process_custom(custom)\n\u001b[0;32m    216\u001b[0m \u001b[39mreturn\u001b[39;00m cm\u001b[39m.\u001b[39mSoupSieve(\n\u001b[0;32m    217\u001b[0m     pattern,\n\u001b[0;32m    218\u001b[0m     CSSParser(\n\u001b[0;32m    219\u001b[0m         pattern,\n\u001b[0;32m    220\u001b[0m         custom\u001b[39m=\u001b[39;49mcustom_selectors,\n\u001b[0;32m    221\u001b[0m         flags\u001b[39m=\u001b[39;49mflags\n\u001b[1;32m--> 222\u001b[0m     )\u001b[39m.\u001b[39;49mprocess_selectors(),\n\u001b[0;32m    223\u001b[0m     namespaces,\n\u001b[0;32m    224\u001b[0m     custom,\n\u001b[0;32m    225\u001b[0m     flags\n\u001b[0;32m    226\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py:1159\u001b[0m, in \u001b[0;36mCSSParser.process_selectors\u001b[1;34m(self, index, flags)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_selectors\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, flags: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ct\u001b[39m.\u001b[39mSelectorList:\n\u001b[0;32m   1157\u001b[0m     \u001b[39m\"\"\"Process selectors.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_selectors(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselector_iter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpattern), index, flags)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py:985\u001b[0m, in \u001b[0;36mCSSParser.parse_selectors\u001b[1;34m(self, iselector, index, flags)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 985\u001b[0m         key, m \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iselector)\n\u001b[0;32m    987\u001b[0m         \u001b[39m# Handle parts\u001b[39;00m\n\u001b[0;32m    988\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mat_rule\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\soupsieve\\css_parser.py:1152\u001b[0m, in \u001b[0;36mCSSParser.selector_iter\u001b[1;34m(self, pattern)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInvalid character \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m position \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(c, index)\n\u001b[1;32m-> 1152\u001b[0m         \u001b[39mraise\u001b[39;00m SelectorSyntaxError(msg, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpattern, index)\n\u001b[0;32m   1153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m## END PARSING\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mSelectorSyntaxError\u001b[0m: Malformed attribute selector at position 1\n  line 1:\np[datetime^=2021]\n ^"
     ]
    }
   ],
   "source": [
    "time = soup.select(\"p[datetime^=2021]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "d. Take the lists that you have created and combine them into a pandas DataFrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m data_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m: jobtitles_text,\n\u001b[0;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcompany\u001b[39m\u001b[39m'\u001b[39m: companies_text,\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m'\u001b[39m: locations_text,\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      7\u001b[0m \u001b[39m# create the DataFrame\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m listings \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data_dict)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    'title': jobtitles_text,\n",
    "    'company': companies_text,\n",
    "    'location': locations_text,\n",
    "}\n",
    "\n",
    "# create the DataFrame\n",
    "listings = pd.DataFrame(data_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Next, add a column that contains the url for the \"Apply\" button. Try this in two ways.   \n",
    "    a. First, use the BeautifulSoup find_all method to extract the urls.  \n",
    "    b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job.\n",
    "    \n",
    "3. Finally, we want to get the job description text for each job.  \n",
    "    a. Start by looking at the page for the first job, https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job description paragraph.  \n",
    "    b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. For example, if you input \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\".  \n",
    "    c. Use the [.apply method](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) on the url column you created above to retrieve the description text for all of the jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
